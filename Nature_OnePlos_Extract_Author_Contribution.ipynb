{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Nature_Extract_Author_Contribution**"
      ],
      "metadata": {
        "id": "CimmbW-TgyHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import files\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import io\n",
        "import os"
      ],
      "metadata": {
        "id": "y_z7VHFUgxjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6t4LnkuvQ2W"
      },
      "outputs": [],
      "source": [
        "def get_soup(u):\n",
        "  \"\"\"\n",
        "  Get The HTMl with Beautiful Soup\n",
        "  \"\"\"\n",
        "\n",
        "  s=requests.get(u).content.decode('utf-8')\n",
        "  html=s\n",
        "  soup=BeautifulSoup(html, 'html.parser')\n",
        "  return soup\n",
        "\n",
        "def get_highest_page_count(soup):\n",
        "  \"\"\"\n",
        "  Get The Highest Number Page In The Website. In bottom of the pages there are buttons to move on to the next page.\n",
        "  Thus function return the biggest page number.\n",
        "  \"\"\"\n",
        "\n",
        "  pagination_links = soup.find_all('a',attrs={'class':'c-pagination__link'})\n",
        "  pattern_pagination=r'page=(\\d+)'\n",
        "  max_page=max([int(re.findall(pattern_pagination,str(pagination_links[i]))[0]) for i in range(len(pagination_links))])\n",
        "  return max_page\n",
        "\n",
        "def extract_title_urls_contribution(topics,type_url):\n",
        "  \"\"\"\n",
        "  By each topic and his url I find the titles and articles' urls in each page.\n",
        "  After that, to each article's url, I find:\n",
        "  1.year publication of the article.\n",
        "  2. The contibution of the authors. The contribuion came under the Section: \"Author Information\" under the title \"Contribution\".\n",
        "  3. Author Address. The university and the country that the authors came from.\n",
        "  4.Authors' Names\n",
        "\n",
        "  The function downloads a dictionary by this structure: {topic:\n",
        "                                            {Page Number:\n",
        "                                            {title:..,url:..,contribution:..,year:..,address:..,authors:..}}}\n",
        "  \"\"\"\n",
        "\n",
        "  for topic in topics:\n",
        "    dict_all={}\n",
        "    if type_url=='Nature Communication':\n",
        "       u = \"https://www.nature.com/subjects/\"+topic+\"/ncomms?searchType=journalSearch&sort=PubDate&page=\"\n",
        "    else:\n",
        "      #Nature\n",
        "      u = \"https://www.nature.com/subjects/\"+topic+\"/srep?searchType=journalSearch&sort=PubDate&page=\"\n",
        "\n",
        "    soup=get_soup(u+str(1))\n",
        "    try:\n",
        "      max_page=get_highest_page_count(soup)\n",
        "    except:\n",
        "      max_page=1\n",
        "\n",
        "    for i in range(1,max_page+1):\n",
        "      dict1={'contribution':[],'url':[],'title':[],'authors':[],\"address\":[],'year':[]}\n",
        "      print(i)\n",
        "      if i>1:\n",
        "        soup=get_soup(u+str(i))\n",
        "      l = soup.find_all('a', href=lambda href: href and href.startswith(\"/articles/\"))\n",
        "      #all the urls shows in this pattern\n",
        "      pattern_url = r'href=\"(.*?)\"\\s*itemprop=\"(.*?)\"'\n",
        "      urls=[re.findall(pattern_url, str(l[i]))[0][0] for i in range(len(l))]\n",
        "      titles=[t.text.strip() for t in l]\n",
        "\n",
        "      for j,url in enumerate(urls):\n",
        "        title=titles[j]\n",
        "        url_article=\"https://www.nature.com/\"+topic+url\n",
        "        soup_article=get_soup(url_article)\n",
        "        year=soup_article.find('span', {'data-test': 'article-publication-year'}).text.strip()\n",
        "\n",
        "        try:\n",
        "          #contribution is under the class c-article__sub-heading under h3 tag and shows after <p .....</p>\n",
        "          target_h3 = soup_article.find('h3', class_='c-article__sub-heading', text='Contributions').find_next('p')\n",
        "          authour_contribution=target_h3.text.strip()\n",
        "        except:\n",
        "          authour_contribution=''\n",
        "        try:\n",
        "          address_elements = soup_article.find_all('p', class_='c-article-author-affiliation__address')\n",
        "          author_elements=soup_article.find_all('p', class_='c-article-author-affiliation__authors-list')\n",
        "          authors=[author.text.strip() for author in author_elements]\n",
        "          address=[add.text.strip() for add in address_elements]\n",
        "        except:\n",
        "          authors=[]\n",
        "          address=[]\n",
        "\n",
        "        dict1['contribution']+=[authour_contribution]\n",
        "        dict1['url']+=[url]\n",
        "        dict1['title']+=[title]\n",
        "        dict1['authors']+=[authors]\n",
        "        dict1['address']+=[address]\n",
        "        dict1['year']+=[year]\n",
        "      dict_all[i]=dict1\n",
        "      #after 30 pages download the file. Because the running is over 20 hours, so I divided to batches to save the results\n",
        "      if i%30==0:\n",
        "         with open(topic+\".json\", 'w') as json_file:\n",
        "            json.dump(dict_all, json_file, indent=4)\n",
        "         files.download(topic+\".json\")\n",
        "\n",
        "    with open(topic+\".json\", 'w') as json_file:\n",
        "      json.dump(dict_all, json_file, indent=4)\n",
        "    files.download(topic+\".json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5TXakd2olA"
      },
      "outputs": [],
      "source": [
        "##add the topic Names in the list topics.\n",
        "topics=['ecology']\n",
        "extract_title_urls_contribution(topics,'Nature Communication')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenPlus Corpus**"
      ],
      "metadata": {
        "id": "3zX-_-WI05vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract Zip File From Drive**"
      ],
      "metadata": {
        "id": "6i8NYmjo0Bu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVMN_VwB1BQm",
        "outputId": "6e99f189-dd14-44d0-a256-d67534dac835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/drive/MyDrive/data_mining_project/allofplos.zip'\n",
        "\n",
        "# Specify the path where you want to extract the contents\n",
        "extracted_folder_path = '/content/extracted_folder/'\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all contents to the specified folder\n",
        "    zip_ref.extractall(extracted_folder_path)"
      ],
      "metadata": {
        "id": "kkvySWSo08zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Title:**"
      ],
      "metadata": {
        "id": "T2Q1390aytC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(root):\n",
        "  '''\n",
        "  Each file is XML file. the title tag in the xml file is .//title-group.\n",
        "  The function return full title name\n",
        "  '''\n",
        "  title_group = root.find('.//title-group')\n",
        "  article_title = title_group.find('.//article-title')\n",
        "  title = ET.tostring(article_title, encoding='utf-8').decode('utf-8').strip()\n",
        "  title = re.sub(r'<.*?>', '', title)\n",
        "  return title"
      ],
      "metadata": {
        "id": "0xk2vYlCytC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Year:**"
      ],
      "metadata": {
        "id": "CtfwuJCyyyXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_year(root):\n",
        "  '''\n",
        "  Each file is XML file. the date tag in the xml file is .///pub-date. From the date tag, I extract the year\n",
        "  The function returns the publication year of the article.\n",
        "  '''\n",
        "  date=root.find('.//pub-date')\n",
        "  year=date.find('.//year')\n",
        "  return year.text"
      ],
      "metadata": {
        "id": "r7fZG0AnyyXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Contribution:**"
      ],
      "metadata": {
        "id": "_nmNZmDsy4hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#type1:\n",
        "def get_contribution(root):\n",
        "    '''\n",
        "    Each file is XML file. the contribution tag in the xml file is ..//*[@fn-type='con'. From the contribution tag,\n",
        "    I extract the full text of the contribution\n",
        "    The function returns contribution types for each author.\n",
        "    '''\n",
        "    try:\n",
        "      contribution=ET.tostring(root.findall(\".//*[@fn-type='con']\")[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "    except:\n",
        "      contribution=[]\n",
        "\n",
        "    return contribution"
      ],
      "metadata": {
        "id": "MnhvAILjy4hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Address:**"
      ],
      "metadata": {
        "id": "oBtQbpwGzEE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_address(root):\n",
        "  address=root.findall('.//addr-line')\n",
        "  aff=root.findall('.//aff')\n",
        "  addr_line_texts={}\n",
        "  if len(address)<=2:\n",
        "    try:\n",
        "      addr_line_texts = {'1': ET.tostring(address[0], encoding='utf-8', method='text').decode('utf-8').strip()}\n",
        "    except:\n",
        "      return {}\n",
        "  else:\n",
        "    for i,elem in enumerate(address):\n",
        "      try:\n",
        "        aff_num=ET.tostring(aff[i][0], encoding='utf-8', method='text').decode('utf-8')[0]\n",
        "      except:\n",
        "        aff_num=ET.tostring(aff[i], encoding='utf-8', method='text').decode('utf-8')[0]\n",
        "      if aff_num.isdigit()==True:\n",
        "        addr_line_texts[aff_num]=ET.tostring(elem, encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "\n",
        "  return addr_line_texts"
      ],
      "metadata": {
        "id": "ytXCmwU_zEE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Names:**\n"
      ],
      "metadata": {
        "id": "0uPmDUHPy9nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_names_address(root,contrib_found):\n",
        "  type_xml=1\n",
        "  lst_names_affs=[]\n",
        "  contrib_grpoup_name=root.findall(\".//*[@contrib-type='author']\")\n",
        "  if contrib_grpoup_name==[]:\n",
        "    return lst_names_affs,0\n",
        "  dict_names_address_contrib={'full name':[],'address':[],'roles':[]}\n",
        "  if contrib_grpoup_name[0].findall('.//role')==[]:\n",
        "    type_xml=0\n",
        "    dict_names_address_contrib={'full name':[],'address':[]}\n",
        "    if contrib_found==[]:\n",
        "      return lst_names_affs,type_xml\n",
        "\n",
        "  for i,elem in enumerate(contrib_grpoup_name):\n",
        "    affs=[]\n",
        "    dict_names_address_contrib2={}\n",
        "    roles_to_each_author=[]\n",
        "    first_name=elem.findall('.//surname')\n",
        "    last_names=elem.findall('.//given-names')\n",
        "    if first_name==[]:\n",
        "      continue\n",
        "    aff=elem.findall('.//sup')\n",
        "    if type_xml==1:\n",
        "      roles= elem.findall('.//role')\n",
        "      for role in roles:\n",
        "        roles_to_each_author.append(ET.tostring(role, encoding='utf-8', method='text').decode('utf-8').strip())\n",
        "\n",
        "    addr_line_texts=get_address(root)\n",
        "    if len(aff)>=1:\n",
        "      for j , aff_id in enumerate(aff):\n",
        "        try:\n",
        "          aff_num=ET.tostring(aff_id, encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "          if aff_num=='*':\n",
        "            affs.append(list(addr_line_texts.values()))\n",
        "          else:\n",
        "            affs.append(addr_line_texts[aff_num])\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    elif aff==[]:\n",
        "      if addr_line_texts!={}:\n",
        "        if '1' in addr_line_texts.keys():\n",
        "          affs.append(addr_line_texts['1'])\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "          affs.append(addr_line_texts[ET.tostring(aff[0], encoding='utf-8', method='text').decode('utf-8').strip()])\n",
        "        except:\n",
        "          continue\n",
        "    if last_names==[]:\n",
        "      full_name=ET.tostring(first_name[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "    else:\n",
        "      full_name=ET.tostring(first_name[0], encoding='utf-8', method='text').decode('utf-8').strip()+' '+ET.tostring(last_names[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "\n",
        "    dict_names_address_contrib2['full name']=full_name\n",
        "    dict_names_address_contrib2['address']=affs\n",
        "    if 'roles' in dict_names_address_contrib.keys():\n",
        "      dict_names_address_contrib2['roles']=roles_to_each_author\n",
        "    lst_names_affs.append(dict_names_address_contrib2)\n",
        "  return lst_names_affs,type_xml"
      ],
      "metadata": {
        "id": "3QNxL_4Py9nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Json With Relevant Features:**"
      ],
      "metadata": {
        "id": "A3JMt2uezH8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/extracted_folder/'\n",
        "files = os.listdir(path)\n",
        "dict_files_meta_data={}\n",
        "for i in range(len(files)):\n",
        "  file_name = os.path.join(path, files[i])\n",
        "  with open(file_name, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "  try:\n",
        "    root = ET.fromstring(content)\n",
        "  except:\n",
        "    continue\n",
        "  # Now 'root' is an ElementTree object that you can work with\n",
        "  root = root.find('.//article-meta')\n",
        "  contribution=get_contribution(root)\n",
        "  dict_name_address_role=get_names_address(root,contribution)\n",
        "  if (contribution==[] and dict_name_address_role[1]==0) or (contribution==[] and dict_name_address_role[0]==[]) :\n",
        "    #there is nor contribution in this article\n",
        "    continue\n",
        "  if contribution!=[] and dict_name_address_role[0]==[]:\n",
        "    dict_file={'title':title,'year':year,'contribution':contribution}\n",
        "    dict_files_meta_data[file_name]=dict_file\n",
        "    continue\n",
        "\n",
        "  year=get_year(root)\n",
        "  title=get_title(root)\n",
        "  if 'roles' in dict_name_address_role[0][0].keys():\n",
        "    dict_file={'title':title,'year':year,'name_address_contribution':dict_name_address_role[0]}\n",
        "  else:\n",
        "    dict_file={'title':title,'year':year,'name_address':dict_name_address_role[0],'contribution':contribution}\n",
        "\n",
        "  dict_files_meta_data[file_name]=dict_file\n",
        "\n",
        "with open(\"plosone.json\", 'w') as json_file:\n",
        "    json.dump(dict_files_meta_data, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "nOJ0kxZt2zao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = os.path.join(path, files[43039])\n",
        "with open(file_name, 'r') as file:\n",
        "    content = file.read()\n"
      ],
      "metadata": {
        "id": "otAGrA4P4sUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(content)"
      ],
      "metadata": {
        "id": "UdSt8rP3Cw6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking:"
      ],
      "metadata": {
        "id": "8nz40gTAzOle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "root = ET.fromstring(content)\n",
        "# Now 'root' is an ElementTree object that you can work with\n",
        "root = root.find('.//article-meta')"
      ],
      "metadata": {
        "id": "hc72jw3Jz6uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Title:"
      ],
      "metadata": {
        "id": "REFhVj2SZbQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(root):\n",
        "  title_group = root.find('.//title-group')\n",
        "  article_title = title_group.find('.//article-title')\n",
        "  title = ET.tostring(article_title, encoding='utf-8').decode('utf-8').strip()\n",
        "  title = re.sub(r'<.*?>', '', title)\n",
        "  return title"
      ],
      "metadata": {
        "id": "CWfLUPOivYGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_title(root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M_APlSRl2usA",
        "outputId": "848c4e79-3cb2-4878-eaed-af0e1914572f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Common Mental Disorders Associated with Tuberculosis: A Matched Case-Control Study'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 713
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Year:"
      ],
      "metadata": {
        "id": "XAoNCL3HZZGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_year(root):\n",
        "  date=root.find('.//pub-date')\n",
        "  year=date.find('.//year')\n",
        "  return year.text"
      ],
      "metadata": {
        "id": "lZ5KCOOJTf3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_year(root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8Qi3fMcY225W",
        "outputId": "92503cac-4dbd-4f09-8d4c-1892814109ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2014'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 714
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contribution:"
      ],
      "metadata": {
        "id": "OMlUD9oMvrhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#type1:\n",
        "def get_contribution(root):\n",
        "    try:\n",
        "      contribution=ET.tostring(root.findall(\".//*[@fn-type='con']\")[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "    except:\n",
        "      contribution=[]\n",
        "\n",
        "    return contribution"
      ],
      "metadata": {
        "id": "Pzd2HocXvrhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_contribution(root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e75ac3f6-47f0-482e-e282-26b6ec10ecc4",
        "id": "ebxxH-BtvrhP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Conceived and designed the experiments: SMP DNS MLB LCR. Performed the experiments: GSA SMP DNS JMM LCR MLB. Analyzed the data: GSA SMP DNS JMM LCR MLB. Contributed reagents/materials/analysis tools: GSA SMP DNS JMM LCR MLB. Wrote the paper: GSA SMP DNS LCR MLB.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 715
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Names:\n"
      ],
      "metadata": {
        "id": "kLbLKDeeZWd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_names_address(root,contrib_found):\n",
        "  type_xml=1\n",
        "  lst_names_affs=[]\n",
        "  contrib_grpoup_name=root.findall(\".//*[@contrib-type='author']\")\n",
        "  if contrib_grpoup_name==[]:\n",
        "    return lst_names_affs,0\n",
        "  dict_names_address_contrib={'full name':[],'address':[],'roles':[]}\n",
        "  if contrib_grpoup_name[0].findall('.//role')==[]:\n",
        "    type_xml=0\n",
        "    dict_names_address_contrib={'full name':[],'address':[]}\n",
        "    if contrib_found==[]:\n",
        "      return lst_names_affs,type_xml\n",
        "\n",
        "  for i,elem in enumerate(contrib_grpoup_name):\n",
        "    affs=[]\n",
        "    dict_names_address_contrib2={}\n",
        "    roles_to_each_author=[]\n",
        "    first_name=elem.findall('.//surname')\n",
        "    last_names=elem.findall('.//given-names')\n",
        "    if first_name==[]:\n",
        "      continue\n",
        "    aff=elem.findall('.//sup')\n",
        "    if type_xml==1:\n",
        "      roles= elem.findall('.//role')\n",
        "      for role in roles:\n",
        "        roles_to_each_author.append(ET.tostring(role, encoding='utf-8', method='text').decode('utf-8').strip())\n",
        "\n",
        "    addr_line_texts=get_address(root)\n",
        "    if len(aff)>=1:\n",
        "      for j , aff_id in enumerate(aff):\n",
        "        try:\n",
        "          aff_num=ET.tostring(aff_id, encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "          if aff_num=='*':\n",
        "            affs.append(list(addr_line_texts.values()))\n",
        "          else:\n",
        "            affs.append(addr_line_texts[aff_num])\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    elif aff==[]:\n",
        "      if addr_line_texts!={}:\n",
        "        if '1' in addr_line_texts.keys():\n",
        "          affs.append(addr_line_texts['1'])\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "          affs.append(addr_line_texts[ET.tostring(aff[0], encoding='utf-8', method='text').decode('utf-8').strip()])\n",
        "        except:\n",
        "          continue\n",
        "    if last_names==[]:\n",
        "      full_name=ET.tostring(first_name[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "    else:\n",
        "      full_name=ET.tostring(first_name[0], encoding='utf-8', method='text').decode('utf-8').strip()+' '+ET.tostring(last_names[0], encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "\n",
        "    dict_names_address_contrib2['full name']=full_name\n",
        "    dict_names_address_contrib2['address']=affs\n",
        "    if 'roles' in dict_names_address_contrib.keys():\n",
        "      dict_names_address_contrib2['roles']=roles_to_each_author\n",
        "    lst_names_affs.append(dict_names_address_contrib2)\n",
        "  return lst_names_affs,type_xml"
      ],
      "metadata": {
        "id": "EmGQgg3ir_D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_names_address(root,get_contribution(root))\n"
      ],
      "metadata": {
        "id": "PTezkpWu3B9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Address:"
      ],
      "metadata": {
        "id": "s7aWACTwaKO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_address(root):\n",
        "  address=root.findall('.//addr-line')\n",
        "  aff=root.findall('.//aff')\n",
        "  addr_line_texts={}\n",
        "  if len(address)<=2:\n",
        "    try:\n",
        "      addr_line_texts = {'1': ET.tostring(address[0], encoding='utf-8', method='text').decode('utf-8').strip()}\n",
        "    except:\n",
        "      return {}\n",
        "  else:\n",
        "    for i,elem in enumerate(address):\n",
        "      try:\n",
        "        aff_num=ET.tostring(aff[i][0], encoding='utf-8', method='text').decode('utf-8')[0]\n",
        "      except:\n",
        "        aff_num=ET.tostring(aff[i], encoding='utf-8', method='text').decode('utf-8')[0]\n",
        "      if aff_num.isdigit()==True:\n",
        "        addr_line_texts[aff_num]=ET.tostring(elem, encoding='utf-8', method='text').decode('utf-8').strip()\n",
        "\n",
        "  return addr_line_texts"
      ],
      "metadata": {
        "id": "Lub7KWJgeEsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_address(root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTzAdqVb2_J2",
        "outputId": "394b5a6e-be0e-433f-86a6-9831209705c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2': 'Departamento de Medicina Interna, School of Medicine and Public Health, Bahia, Brazil',\n",
              " '3': 'Department of Epidemiology and Population Health, London School of Hygiene and Tropical Medicine, London, United Kingdom'}"
            ]
          },
          "metadata": {},
          "execution_count": 727
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CimmbW-TgyHK",
        "3zX-_-WI05vy",
        "6i8NYmjo0Bu-",
        "T2Q1390aytC1",
        "CtfwuJCyyyXX",
        "_nmNZmDsy4hZ",
        "oBtQbpwGzEE4",
        "0uPmDUHPy9nV",
        "8nz40gTAzOle",
        "REFhVj2SZbQO",
        "XAoNCL3HZZGk",
        "OMlUD9oMvrhP",
        "kLbLKDeeZWd-",
        "s7aWACTwaKO0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}